Формулировка задачи:

Большинство веб-страниц сейчас перегружено всевозможной рекламой... Наша задача «вытащить» из веб-страницы только полезную информацию, отбросив весь «мусор» (навигацию, рекламу и тд).

Полученный текст нужно отформатировать для максимально комфортного чтения в любом текстовом редакторе. Правила форматирования: ширина строки не больше 80 символов (если больше, переносим по словам), абзацы и заголовки отбиваются пустой строкой. Если в тексте встречаются ссылки, то URL вставить в текст в квадратных скобках. Остальные правила на ваше усмотрение.

Программа оформляется в виде утилиты командной строки, которой в качестве параметра указывается произвольный URL. Она извлекает по этому URL страницу, обрабатывает ее и формирует текстовый файл с текстом статьи, представленной на данной странице.

В качестве примера можно взять любую статью на lenta.ru, gazeta.ru и т.д.

Алгоритм должен быть максимально универсальным, то есть работать на большинстве сайтов.

Усложнение задачи 1: Имя выходного файла должно формироваться автоматически по URL.

Примерно так:

http://lenta.ru/news/2013/03/dtp/index.html => [CUR_DIR]/lenta.ru/news/2013/03/dtp/index.txt

Усложнение задачи 2: Программа должна поддаваться настройке – в отдельном файле/файлах задаются шаблоны обработки страниц.

Требования к выполнению задачи:
1) Задача выполняется на С++|Python с использованием классов. Не должно использоваться сторонних библиотек, впрямую решающих задачу.

2) Предпочтительная среда выполнения – MS Windows.

3) Решение должно состоять из документа, описывающего алгоритм, исходных кодов программы, исполняемого модуля.

4) Приложите список URL, на которых вы проверяли свое решение. И результаты проверки.

5) Желательно указать направление дальнейшего улучшения/развития программы.

Решение:

Задание выполнено на python. Для создания дерева html документа и удобного поиска тегов по нему использовалась библиотека lxml (остальные используемые бибилиотеки указаны в requirements.txt). В исполняемом файле парсится строка из консоли, затем по полученному url скачивается html страница, обрабатывается и сохраняется в файл.

Файлы проекта:

1) parse.py - описан класс для обработки html. 
2) main.py - главный исполняемый файл
3) inout - описаны методы для парсинга консоли, получения html и сохранения файла
4) config - параметры обработки текста статьи
5) requirements.txt - список используемых библиотек

Алгоритм парсинга:

С помощью xpath находятся нужные элементы содержащие текст параграфов и заголовков. Затем происходит последовательное прохождение по выбранным элементам с вхождением во вложенные элементы (для парсинга ссылок и текста в них) и из них собирается текст. Затем когда доходим до последнего элемента на данном уровне вложенности параграфов цикл сбора текста завершается. Далее он склеивается и обрабатывается с учетом праметров (нужны ли ссылки, ширина строки, заголовки). 

Усложнение 1:

Добавлено автоматическое сохранение файла в директорию по url.

Усложнение 2:

Добавлен config и 4 параметра для настройки обработки страниц.

Результаты:

Проверялись следующие источники:

https://lenta.ru/news/2023/04/02/edinenie/
https://m.gazeta.ru/sport/news/2023/04/02/20115829.shtml
https://m.gazeta.ru/social/photo/u_apollona_ruhnula_krysha_posledstviya_tornado_v_ssha.shtml
https://www.rbc.ru/rbcfreenews/6428e5799a7947d8182c6660?utm_source=yxnews&utm_medium=desktop

Статьи и результы их обработки представлены в папке samples.
